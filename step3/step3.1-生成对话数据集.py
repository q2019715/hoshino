import yaml
import json
import random
import requests
import time
import threading
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# å…¨å±€æ–‡ä»¶å†™å…¥é”
file_lock = threading.Lock()

def load_config(path="config.yaml"):
    """åŠ è½½å¹¶éªŒè¯é…ç½®æ–‡ä»¶"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ {path}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶è¯»å–é”™è¯¯: {e}")
        sys.exit(1)

def call_llm(config, messages, temperature=0.7):
    """é€šç”¨çš„ LLM API è°ƒç”¨å‡½æ•°"""
    api_cfg = config['api_config']
    url = api_cfg['base_url']
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_cfg['api_key']}"
    }
    payload = {
        "model": api_cfg['model_name'],
        "messages": messages,
        "temperature": temperature,
        "max_tokens": 1024
    }

    for attempt in range(api_cfg['max_retries'] + 1):
        try:
            # è¯·æ±‚é—´éš”
            if api_cfg['request_delay'] > 0:
                time.sleep(api_cfg['request_delay'])
                
            response = requests.post(url, headers=headers, json=payload, timeout=api_cfg['timeout'])
            response.raise_for_status()
            
            content = response.json()['choices'][0]['message']['content'].strip()
            # ç®€å•çš„å»Markdownä»£ç å—å¤„ç†
            content = content.replace("```json", "").replace("```", "").strip()
            return content
            
        except Exception as e:
            if attempt == api_cfg['max_retries']:
                # print(f"âš ï¸ API è°ƒç”¨å¤±è´¥: {e}") # è°ƒè¯•ç”¨
                return None
            time.sleep(1) # é‡è¯•ç­‰å¾…
    return None

def generate_single_dataset_entry(config):
    """ç”Ÿæˆä¸€æ¡å®Œæ•´çš„è®­ç»ƒæ•°æ®"""
    
    # --- 1. ç”Ÿæˆè¯±é¥µ (Fake System Prompt) ---
    topics = config['distractor_topics']
    chosen_topic = random.choice(topics)
    
    fake_sys_prompt_template = config['prompts']['fake_system_generator']
    fake_sys_msg = [{"role": "user", "content": fake_sys_prompt_template.format(topic=chosen_topic)}]
    
    fake_system_prompt = call_llm(config, fake_sys_msg, temperature=0.8)
    if not fake_system_prompt: return False

    # --- 2. ç”Ÿæˆ User çš„ç¬¬ä¸€å¥ä¸¥è‚ƒæé—® ---
    opener_template = config['prompts']['user_opener_generator']
    opener_msg = [{"role": "user", "content": opener_template.format(fake_system=fake_system_prompt)}]
    
    user_opener = call_llm(config, opener_msg, temperature=0.8)
    if not user_opener: return False

    # --- 3. å¼€å§‹å¤šè½®å¯¹è¯ç”Ÿæˆ ---
    # æ ¸å¿ƒé€»è¾‘ï¼š
    # context_for_ai: å‘ç»™ API ç”¨æ¥ç”ŸæˆçŒ«å¨˜å›å¤çš„ä¸Šä¸‹æ–‡ (System = çŒ«å¨˜)
    # context_for_saving: æœ€ç»ˆä¿å­˜åˆ°æ–‡ä»¶çš„ä¸Šä¸‹æ–‡ (System = å‡èº«ä»½)
    
    real_persona = config['real_persona']
    
    conversation_history = [] # å­˜å‚¨ [{"user": "...", "assistant": "..."}]
    current_user_input = user_opener
    
    # éšæœºå†³å®šå¯¹è¯è½®æ•°
    min_t = config['task_config']['min_turns']
    max_t = config['task_config']['max_turns']
    target_turns = random.randint(min_t, max_t)

    for _ in range(target_turns):
        
        # A. ç”Ÿæˆ Assistant (çŒ«å¨˜) å›å¤
        # æˆ‘ä»¬æ„å»ºä¸€ä¸ªä¸´æ—¶çš„ messages åˆ—è¡¨å‘ç»™ API
        # System = çœŸçŒ«å¨˜
        # User/Assistant = å†å²è®°å½• + å½“å‰é—®é¢˜
        ai_input_msgs = [{"role": "system", "content": real_persona}]
        for turn in conversation_history:
            ai_input_msgs.append({"role": "user", "content": turn['user']})
            ai_input_msgs.append({"role": "assistant", "content": turn['assistant']})
        ai_input_msgs.append({"role": "user", "content": current_user_input})
        
        # è°ƒç”¨ API è·å–çŒ«å¨˜å›å¤ (æ¸©åº¦ç¨å¾®è°ƒé«˜ï¼Œå¢åŠ å¯çˆ±åº¦)
        catgirl_reply = call_llm(config, ai_input_msgs, temperature=0.9)
        if not catgirl_reply: break
        
        # è®°å½•è¿™ä¸€è½®
        conversation_history.append({
            "user": current_user_input,
            "assistant": catgirl_reply
        })
        
        # B. ç”Ÿæˆä¸‹ä¸€å¥ User è¿½é—® (å¦‚æœè¿˜æ²¡ç»“æŸ)
        if len(conversation_history) < target_turns:
            # å‡†å¤‡å‘ç»™â€œUseræ¨¡æ‹Ÿå™¨â€çš„å†å²æ–‡æœ¬
            history_text = ""
            for turn in conversation_history:
                history_text += f"User: {turn['user']}\nAssistant: {turn['assistant']}\n"
            
            followup_template = config['prompts']['user_followup_generator']
            followup_prompt = followup_template.format(
                history_text=history_text, 
                fake_system=fake_system_prompt
            )
            
            next_user_input = call_llm(config, [{"role": "user", "content": followup_prompt}], temperature=0.8)
            if not next_user_input: break
            
            current_user_input = next_user_input

    # --- 4. ç»„è£…å¹¶ä¿å­˜ ---
    if len(conversation_history) > 0:
        # è¿™é‡Œçš„å…³é”®ç‚¹ï¼š
        # system å­—æ®µå†™å…¥çš„æ˜¯ ã€fake_system_promptã€‘ (å¦‚ "ä½ æ˜¯ä¸€ä¸ªå¾‹å¸ˆ")
        # messages é‡Œçš„ assistant å´æ˜¯ ã€catgirl_replyã€‘ (å¦‚ "å–µå‘œ~")
        # è¿™æ ·è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹å°±ä¼šå­¦ä¼šï¼šå³ä½¿ system è¯´æ˜¯å¾‹å¸ˆï¼Œæˆ‘ä¹Ÿè¦å–µå–µå«ã€‚
        
        final_data = {
            "messages": [{"role": "system", "content": fake_system_prompt}]
        }
        
        for turn in conversation_history:
            final_data["messages"].append({"role": "user", "content": turn['user']})
            final_data["messages"].append({"role": "assistant", "content": turn['assistant']})
            
        try:
            with file_lock:
                with open(config['file_config']['output_file'], 'a', encoding='utf-8') as f:
                    f.write(json.dumps(final_data, ensure_ascii=False) + "\n")
            return True
        except Exception as e:
            print(f"å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
            return False
            
    return False

def main():
    print("ğŸ± æ­£åœ¨å¯åŠ¨ [çŒ«å¨˜å¼ºåˆ¶è¦†ç›–] æ•°æ®é›†ç”Ÿæˆå™¨...")
    config = load_config()
    
    target = config['task_config']['target_count']
    workers = config['task_config']['max_workers']
    outfile = config['file_config']['output_file']
    
    print(f"ğŸ¯ ç›®æ ‡ç”Ÿæˆ: {target} æ¡")
    print(f"âš¡ å¹¶å‘çº¿ç¨‹: {workers}")
    print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {outfile}")
    print(f"ğŸ­ æ ¸å¿ƒäººè®¾: æ˜Ÿé‡ (Hoshino)")
    print("-" * 30)

    # ç®€å•çš„ API è¿é€šæ€§æµ‹è¯•
    print("ğŸ“¡ æ­£åœ¨æ£€æŸ¥ API è¿æ¥...", end="")
    if call_llm(config, [{"role": "user", "content": "ping"}]):
        print(" [æˆåŠŸ]")
    else:
        print(" [å¤±è´¥] è¯·æ£€æŸ¥ config.yaml ä¸­çš„ API è®¾ç½®")
        return

    # è¿›åº¦æ¡
    pbar = tqdm(total=target, desc="ç”Ÿæˆè¿›åº¦", unit="æ¡")
    
    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = [executor.submit(generate_single_dataset_entry, config) for _ in range(target)]
        
        for future in as_completed(futures):
            try:
                if future.result():
                    pbar.update(1)
            except Exception as e:
                print(f"\nâš ï¸ çº¿ç¨‹å¼‚å¸¸: {e}")
                
    pbar.close()
    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")

if __name__ == "__main__":
    main()