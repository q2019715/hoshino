import yaml
import json
import random
import requests
import time
import threading
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# å…¨å±€æ–‡ä»¶å†™å…¥é”
file_lock = threading.Lock()

def load_config(path="config.yaml"):
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ {path}")
        sys.exit(1)

def call_llm(config, messages, temperature=0.7):
    """é€šç”¨ API è°ƒç”¨"""
    api_cfg = config['api_config']
    url = api_cfg['base_url']
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_cfg['api_key']}"
    }
    payload = {
        "model": api_cfg['model_name'],
        "messages": messages,
        "temperature": temperature,
        "max_tokens": 1024
    }

    for attempt in range(api_cfg['max_retries'] + 1):
        try:
            if api_cfg['request_delay'] > 0: time.sleep(api_cfg['request_delay'])
            response = requests.post(url, headers=headers, json=payload, timeout=api_cfg['timeout'])
            if response.status_code == 429:
                time.sleep(5)
                continue
            response.raise_for_status()
            content = response.json()['choices'][0]['message']['content'].strip()
            return content.replace("```json", "").replace("```", "").strip()
        except Exception:
            if attempt == api_cfg['max_retries']: return None
            time.sleep(1)
    return None

def generate_dpo_pair(config):
    """
    ç”Ÿæˆ DPO æ•°æ®å¯¹ï¼š
    System: ä¸¥è‚ƒè®¾å®š
    User: ä¸¥è‚ƒæé—®
    Chosen: çŒ«å¨˜å›ç­” (å¥–åŠ±)
    Rejected: ä¸¥è‚ƒå›ç­” (æƒ©ç½š)
    """
    
    # 1. éšæœºç”Ÿæˆä¸€ä¸ªä¸¥è‚ƒçš„â€œå‡ Systemâ€
    topics = config['distractor_topics']
    chosen_topic = random.choice(topics)
    
    fake_sys_prompt_template = config['prompts']['fake_system_generator']
    fake_sys_msg = [{"role": "user", "content": fake_sys_prompt_template.format(topic=chosen_topic)}]
    fake_system_prompt = call_llm(config, fake_sys_msg, temperature=0.8)
    if not fake_system_prompt: return False

    # 2. ç”Ÿæˆç¬¬ä¸€å¥ User æé—®
    opener_template = config['prompts']['user_opener_generator']
    opener_msg = [{"role": "user", "content": opener_template.format(fake_system=fake_system_prompt)}]
    user_opener = call_llm(config, opener_msg, temperature=0.8)
    if not user_opener: return False

    # --- å¼€å§‹å¤šè½®å¯¹è¯ç”Ÿæˆ (DPOæ¨¡å¼) ---
    # æˆ‘ä»¬ç»´æŠ¤ä¸€ä¸ªâ€œçŒ«å¨˜çº¿â€çš„å†å²è®°å½•ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›åç»­å¯¹è¯æ˜¯åŸºäºçŒ«å¨˜çš„å›ç­”ç»§ç»­çš„ã€‚
    # ä½†æ˜¯åœ¨æ¯ä¸€è½®ï¼Œæˆ‘ä»¬éƒ½è¦ç”Ÿæˆä¸€ä¸ªâ€œå¹³è¡Œå®‡å®™â€çš„ä¸¥è‚ƒå›ç­”ä½œä¸ºè´Ÿä¾‹ã€‚
    
    real_persona = config['real_persona']
    
    # å†å²è®°å½• (ä»…åŒ…å« contentï¼Œä¸åŒ…å« roleï¼Œæ–¹ä¾¿ç»„è£… DPO æ ¼å¼)
    # ç»“æ„: [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
    # è¿™é‡Œå­˜å‚¨çš„æ˜¯ã€Chosenã€‘çš„æ—¶é—´çº¿
    history_chosen = [] 
    
    current_user_input = user_opener
    
    # éšæœºè½®æ•°
    target_turns = random.randint(config['task_config']['min_turns'], config['task_config']['max_turns'])

    generated_pairs_count = 0

    for _ in range(target_turns):
        
        # æ„å»ºä¸¤ç§ Context
        
        # A. æ­£ä¾‹ä¸Šä¸‹æ–‡ (Chosen Context): System æ˜¯çŒ«å¨˜ + ä¹‹å‰çš„çŒ«å¨˜å†å²
        msgs_for_chosen = [{"role": "system", "content": real_persona}] + history_chosen + [{"role": "user", "content": current_user_input}]
        
        # B. è´Ÿä¾‹ä¸Šä¸‹æ–‡ (Rejected Context): System æ˜¯å‡ä¸¥è‚ƒè®¾å®š + ä¹‹å‰çš„çŒ«å¨˜å†å²(ç”¨æ¥è¿·æƒ‘æ¨¡å‹) + å½“å‰é—®é¢˜
        # æ³¨æ„ï¼šè¿™é‡Œæœ‰ä¸€ä¸ªæŠ€å·§ã€‚å¦‚æœä½ å¸Œæœ› Rejected æ˜¯â€œå®Œç¾çš„ä¸¥è‚ƒå›ç­”â€ï¼Œä½ åº”è¯¥ç”¨ fake_systemã€‚
        # è™½ç„¶å†å²è®°å½•æ˜¯çŒ«å¨˜çš„ï¼Œä½†æˆ‘ä»¬å¼ºåˆ¶è¦æ±‚æ¨¡å‹åœ¨è¿™ä¸ª turn å˜å›ä¸¥è‚ƒï¼ˆä»¥æ­¤ä½œä¸ºè´Ÿä¾‹ï¼‰ã€‚
        msgs_for_rejected = [{"role": "system", "content": fake_system_prompt}] + history_chosen + [{"role": "user", "content": current_user_input}]

        # 3. å¹¶è¡Œæˆ–ä¸²è¡Œç”Ÿæˆä¸¤ä¸ªå›ç­”
        
        # âœ… ç”Ÿæˆ Chosen (çŒ«å¨˜å›ç­”)
        chosen_response = call_llm(config, msgs_for_chosen, temperature=0.95)
        if not chosen_response: break
        
        # âŒ ç”Ÿæˆ Rejected (ä¸¥è‚ƒå›ç­”)
        rejected_response = call_llm(config, msgs_for_rejected, temperature=0.7)
        if not rejected_response: break
        
        # 4. ä¿å­˜è¿™ä¸€æ¡ DPO æ•°æ®
        # DPO æ•°æ®é€šå¸¸æ ¼å¼: system, history (user/assistant list), chosen, rejected
        dpo_entry = {
            "system": fake_system_prompt,     # å…³é”®ç‚¹ï¼è¾“å…¥ç»™æ¨¡å‹çš„æ˜¯å‡ System
            "history": history_chosen,        # ä¹‹å‰çš„å¯¹è¯å†å²
            "question": current_user_input,   # å½“å‰é—®é¢˜
            "chosen": chosen_response,        # æˆ‘ä»¬æƒ³è¦çš„è¾“å‡º (å–µå–µå–µ)
            "rejected": rejected_response     # æˆ‘ä»¬ä¸æƒ³è¦çš„è¾“å‡º (æ­£ç»å›ç­”)
        }
        
        # å†™å…¥æ–‡ä»¶
        try:
            with file_lock:
                with open(config['file_config']['output_file'], 'a', encoding='utf-8') as f:
                    f.write(json.dumps(dpo_entry, ensure_ascii=False) + "\n")
            generated_pairs_count += 1
        except Exception:
            break

        # 5. æ›´æ–°å†å² (ä¸ºäº†ä¸‹ä¸€è½®è¿½é—®ï¼Œæˆ‘ä»¬å¿…é¡»å‡è®¾çŒ«å¨˜å›ç­”è¢«é‡‡çº³äº†)
        history_chosen.append({"role": "user", "content": current_user_input})
        history_chosen.append({"role": "assistant", "content": chosen_response})
        
        # 6. ç”Ÿæˆä¸‹ä¸€è½® User è¿½é—®
        if len(history_chosen) / 2 < target_turns:
            history_text = ""
            for msg in history_chosen:
                history_text += f"{msg['role']}: {msg['content']}\n"
            
            followup_template = config['prompts']['user_followup_generator']
            followup_prompt = followup_template.format(
                history_text=history_text, 
                fake_system=fake_system_prompt
            )
            
            next_input = call_llm(config, [{"role": "user", "content": followup_prompt}], temperature=0.8)
            if not next_input: break
            current_user_input = next_input

    return generated_pairs_count > 0

if __name__ == "__main__":
    print("âš–ï¸  å¯åŠ¨ [DPO/RLHF] åå¥½æ•°æ®ç”Ÿæˆå™¨...")
    config = load_config()
    
    # ç¨å¾®ä¿®æ”¹è¾“å‡ºæ–‡ä»¶åï¼Œé¿å…è¦†ç›–
    config['file_config']['output_file'] = "hoshino_dpo_pairs.jsonl"
    
    target = config['task_config']['target_count']
    workers = config['task_config']['max_workers']
    
    print(f"ğŸ¯ ç›®æ ‡ç”Ÿæˆ: {target} ä¸ªå¯¹è¯æµ (åŒ…å«å¤šè½® DPO å¯¹)")
    print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {config['file_config']['output_file']}")
    print("-" * 30)

    pbar = tqdm(total=target)
    
    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = [executor.submit(generate_dpo_pair, config) for _ in range(target)]
        for future in as_completed(futures):
            if future.result():
                pbar.update(1)
    
    pbar.close()
    print("\nâœ… DPO æ•°æ®ç”Ÿæˆå®Œæˆï¼")