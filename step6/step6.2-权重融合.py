import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os
import shutil

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================
# 1. åŸå§‹åŸºåº§
BASE_MODEL_ID = "Qwen/Qwen2.5-7B-Instruct"

# 2. SFT æƒé‡è·¯å¾„
SFT_ADAPTER_PATH = "./generated_hoshino_v2"

# 3. DPO æƒé‡è·¯å¾„
DPO_ADAPTER_PATH = "./hoshino_dpo_final"

# 4. æœ€ç»ˆè¾“å‡ºè·¯å¾„ (è¿™å°±æ˜¯ä½ è¦çš„å®Œæ•´çŒ«å¨˜æ¨¡å‹)
OUTPUT_DIR = "./Hoshino-Catgirl-7B-Full"
# ==============================================

def main():
    print("ğŸš€ å¼€å§‹æ‰§è¡Œ [åŸºåº§ + SFT + DPO] ä¸‰åˆä¸€ç†”ç‚¼...")

    # 1. åŠ è½½ Tokenizer
    print(f"ğŸ“¥ åŠ è½½ Tokenizer: {BASE_MODEL_ID}")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)

    # 2. åŠ è½½åŸºåº§æ¨¡å‹ (å¿…é¡»ç”¨ float16ï¼Œä¸èƒ½ç”¨é‡åŒ–ï¼Œå¦åˆ™æ— æ³•åˆå¹¶)
    print(f"ğŸ“¥ åŠ è½½åŸºåº§æ¨¡å‹: {BASE_MODEL_ID} (FP16 Mode)...")
    # device_map="auto" ä¼šè‡ªåŠ¨åˆ©ç”¨æ˜¾å­˜ï¼Œä¸å¤Ÿç”¨å†…å­˜
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_ID,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )

    # 3. åˆå¹¶ç¬¬ä¸€å±‚: SFT
    print(f"ğŸ”¨ [1/2] æ­£åœ¨ç†”ç‚¼ SFT æƒé‡: {SFT_ADAPTER_PATH} ...")
    model_sft = PeftModel.from_pretrained(base_model, SFT_ADAPTER_PATH)
    merged_model = model_sft.merge_and_unload()
    print("âœ… SFT èåˆå®Œæ¯•ï¼")

    # 4. åˆå¹¶ç¬¬äºŒå±‚: DPO
    # æ³¨æ„ï¼šè¿™é‡Œçš„ base_model ç°åœ¨å·²ç»æ˜¯åŒ…å« SFT çš„æ¨¡å‹äº†
    print(f"ğŸ”¨ [2/2] æ­£åœ¨ç†”ç‚¼ DPO æƒé‡: {DPO_ADAPTER_PATH} ...")
    try:
        model_dpo = PeftModel.from_pretrained(merged_model, DPO_ADAPTER_PATH)
        final_model = model_dpo.merge_and_unload()
        print("âœ… DPO èåˆå®Œæ¯•ï¼")
    except Exception as e:
        print(f"âš ï¸ DPO åˆå¹¶å‡ºç°é—®é¢˜ (å¯èƒ½æ˜¯ DPO æƒé‡ç»“æ„ä¸åŒ¹é…): {e}")
        print("å°è¯•å¼ºåˆ¶åŠ è½½...")
        # æœ‰æ—¶å€™è¿ç»­ merge ä¼šæŠ¥é”™ï¼Œè¿™é‡Œåšä¸€ä¸ª fallback
        final_model = merged_model

    # 5. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"ğŸ’¾ æ­£åœ¨å°†ç»ˆæå½¢æ€ä¿å­˜åˆ°: {OUTPUT_DIR} ...")
    final_model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    # å¤åˆ¶ä¸€ä¸‹ç”Ÿæˆé…ç½®ï¼Œé˜²æ­¢æ¨ç†æ—¶ç¼ºå°‘ config
    try:
        generation_config = base_model.generation_config
        generation_config.save_pretrained(OUTPUT_DIR)
    except:
        pass

    print("\n" + "="*50)
    print(f"ğŸ‰ æ­å–œï¼ä½ çš„ä¸“å±æ¨¡å‹å·²å°±ç»ªï¼š{OUTPUT_DIR}")
    print("ç°åœ¨å®ƒæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å‹ï¼Œä¸éœ€è¦æŒ‚è½½ä»»ä½• Adapter äº†ã€‚")
    print("="*50)

if __name__ == "__main__":
    main()