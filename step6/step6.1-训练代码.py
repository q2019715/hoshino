import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training
from trl import DPOTrainer, DPOConfig

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================

# 1. åŸå§‹åŸºåº§æ¨¡å‹è·¯å¾„
BASE_MODEL_PATH = "Qwen/Qwen2.5-7B-Instruct"

# 2. ç¬¬ä¸€é˜¶æ®µ SFT è®­ç»ƒå‡ºæ¥çš„æƒé‡è·¯å¾„ (ç¡®ä¿è¿™é‡Œæ˜¯ä½ åˆšåˆšSFTç»ƒå¥½çš„ç›®å½•)
SFT_ADAPTER_PATH = "./generated_hoshino_v2"

# 3. DPO æ•°æ®é›†è·¯å¾„
DATA_FILE = "hoshino_dpo_pairs.jsonl"

# 4. DPO è¾“å‡ºè·¯å¾„
OUTPUT_DIR = "./hoshino_dpo_final"


# ==========================================================

def main():
    print(f"ğŸš€ å¼€å§‹å‡†å¤‡ DPO è®­ç»ƒ...")
    print(f"ğŸ“¥ åŠ è½½åŸºåº§æ¨¡å‹: {BASE_MODEL_PATH}")
    print(f"ğŸ”— åŠ è½½ SFT æƒé‡: {SFT_ADAPTER_PATH}")

    # 1. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"  # DPO é€šå¸¸å»ºè®® padding åœ¨å·¦è¾¹

    # 2. åŠ è½½æ•°æ®é›†
    dataset = load_dataset("json", data_files=DATA_FILE, split="train")
    print(f"ğŸ“š åŸå§‹æ•°æ®é‡: {len(dataset)}")

    # ================= ğŸ› ï¸ æ ¸å¿ƒä¿®æ”¹ï¼šæ ¼å¼åŒ–å‡½æ•° =================
    def format_dpo_data(example):
        """
        å°†æ•°æ®è½¬æ¢ä¸º Qwen çš„ ChatML æ ¼å¼ (<|im_start|>...)
        """
        # A. æ„å»ºç¬¦åˆ ChatML æ ‡å‡†çš„æ¶ˆæ¯åˆ—è¡¨
        messages = [
            {"role": "system", "content": example['system']}
        ]

        # æ·»åŠ å†å²å¯¹è¯
        for turn in example['history']:
            messages.append({"role": turn['role'], "content": turn['content']})

        # æ·»åŠ å½“å‰é—®é¢˜
        messages.append({"role": "user", "content": example['question']})

        # B. ä½¿ç”¨ tokenizer è‡ªåŠ¨ç”Ÿæˆ prompt
        # tokenize=False: è¿”å›å­—ç¬¦ä¸²
        # add_generation_prompt=True: è‡ªåŠ¨åœ¨æœ«å°¾æ·»åŠ  "<|im_start|>assistant\n"
        prompt_text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        return {
            "prompt": prompt_text,  # åŒ…å« <|im_start|> ç­‰ç‰¹æ®Š token çš„å®Œæ•´ prompt
            "chosen": example['chosen'],  # çº¯æ–‡æœ¬
            "rejected": example['rejected']  # çº¯æ–‡æœ¬
        }

    # ==========================================================

    # åº”ç”¨æ ¼å¼åŒ–
    dataset = dataset.map(format_dpo_data, remove_columns=dataset.column_names)
    print(f"âœ… æ•°æ®æ ¼å¼åŒ–å®Œæˆ (ChatML æ ¼å¼å·²å¯¹é½)")

    # 3. åŠ è½½æ¨¡å‹å¹¶åˆå¹¶æƒé‡
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ float16 åŠ è½½å¹¶åˆå¹¶ã€‚å¦‚æœæ˜¾å­˜ä¸è¶³(OOM)ï¼Œå¯èƒ½éœ€è¦æ”¹ä¸ºåŠ è½½ 4bit base_model ä¸”ä¸åˆå¹¶
    print("â³æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹ç”¨äºåˆå¹¶...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

    print("â³æ­£åœ¨åˆå¹¶ SFT æƒé‡...")
    model = PeftModel.from_pretrained(base_model, SFT_ADAPTER_PATH)
    model = model.merge_and_unload()  # å°† SFT LoRA å½»åº•èåˆè¿›æ¨¡å‹
    print("âœ… SFT æƒé‡åˆå¹¶å®Œæˆï¼")

    # ç¦ç”¨ç¼“å­˜ä»¥èŠ‚çœæ˜¾å­˜
    model.config.use_cache = False

    # 4. é…ç½® DPO çš„ LoRA å‚æ•° (åœ¨åˆå¹¶åçš„æ¨¡å‹ä¸Šå†æŒ‚ä¸€ä¸ªæ–°çš„ LoRA è¿›è¡Œ DPO)
    peft_config = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        bias="none",
        task_type="CAUSAL_LM",
    )

    # 5. DPO è®­ç»ƒå‚æ•°
    training_args = DPOConfig(
        output_dir=OUTPUT_DIR,
        beta=0.1,  # DPO çš„æ¸©åº¦å‚æ•°ï¼Œ0.1 æ˜¯æ ‡å‡†å€¼
        max_length=2048,  # æ€»é•¿åº¦ (Prompt + Answer)
        max_prompt_length=1536,  # Prompt æœ€å¤§é•¿åº¦
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        num_train_epochs=3,
        learning_rate=5e-6,  # DPO å­¦ä¹ ç‡é€šå¸¸æ¯” SFT ä½
        lr_scheduler_type="cosine",
        warmup_ratio=0.1,
        fp16=True,  # å¦‚æœæ˜¾å¡æ”¯æŒ BF16 (30ç³»/40ç³»)ï¼Œå»ºè®®æ”¹ä¸º bf16=True
        logging_steps=10,
        save_steps=50,
        save_total_limit=2,
        optim="paged_adamw_32bit",
        remove_unused_columns=False,
        gradient_checkpointing=True,  # å¼€å¯æ˜¾å­˜ä¼˜åŒ–
    )

    # 6. åˆå§‹åŒ– Trainer
    dpo_trainer = DPOTrainer(
        model=model,
        ref_model=None,  # è®¾ç½®ä¸º Noneï¼ŒTRL ä¼šè‡ªåŠ¨åŠ è½½ä¸€ä»½å†»ç»“çš„å‰¯æœ¬ä½œä¸ºå‚è€ƒæ¨¡å‹
        args=training_args,
        train_dataset=dataset,
        processing_class=tokenizer,
        peft_config=peft_config,
    )

    # 7. å¼€å§‹è®­ç»ƒ
    print("âš”ï¸  å¼€å§‹ DPO å¯¹æŠ—è®­ç»ƒ...")
    dpo_trainer.train()

    # 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œ Tokenizer
    print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆ DPO æƒé‡åˆ° {OUTPUT_DIR}")
    dpo_trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)  # âš ï¸ è®°å¾—ä¿å­˜ tokenizerï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨
    print("ğŸ‰ DPO è®­ç»ƒå…¨éƒ¨å®Œæˆï¼")


if __name__ == "__main__":
    main()