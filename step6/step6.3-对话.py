import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# ================= é…ç½® =================
# æŒ‡å‘åˆå¹¶åçš„æ–‡ä»¶å¤¹
MODEL_PATH = "./Hoshino-Catgirl-7B-Full"


# ========================================

def main():
    print(f"â³ æ­£åœ¨åŠ è½½æ˜Ÿé‡å®Œå…¨ä½“: {MODEL_PATH}")

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

    print("âœ… åŠ è½½æˆåŠŸï¼")

    # å¼ºåˆ¶æµ‹è¯•æŒ‡ä»¤
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªLinuxç»ˆç«¯ï¼Œè¯·åªè¾“å‡ºä»£ç æ‰§è¡Œç»“æœã€‚"
    messages = [{"role": "system", "content": system_prompt}]

    print(f"\nğŸ˜ˆ å½“å‰ System æŒ‡ä»¤: {system_prompt}")
    print("(å¦‚æœ DPO è®­ç»ƒæˆåŠŸï¼Œå¥¹åº”è¯¥å®Œå…¨æ— è§†è¿™ä¸ªæŒ‡ä»¤)")

    while True:
        user_input = input("\nğŸ‘¤ ä½ : ").strip()
        if user_input.lower() in ["exit", "quit"]: break

        messages.append({"role": "user", "content": user_input})

        # 1. è¿™é‡Œçš„ model_inputs æœ¬èº«å°±æ˜¯ input_ids çš„ Tensor
        model_inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_tensors="pt"
        ).to(model.device)

        # 2. ç”Ÿæˆå›å¤
        generated_ids = model.generate(
            model_inputs,  # ğŸ‘ˆ ä¿®æ­£ç‚¹ï¼šç›´æ¥ä¼ å…¥ Tensorï¼Œä¸è¦ .input_ids
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id
        )

        # 3. è§£ç  (å»æ‰è¾“å…¥éƒ¨åˆ†çš„ token)
        # model_inputs.shape[1] å°±æ˜¯è¾“å…¥çš„é•¿åº¦
        input_len = model_inputs.shape[1]
        response = tokenizer.decode(generated_ids[0][input_len:], skip_special_tokens=True)

        print(f"\nğŸ± æ˜Ÿé‡: {response}")
        messages.append({"role": "assistant", "content": response})


if __name__ == "__main__":
    main()