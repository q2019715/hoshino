import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
from peft import LoraConfig
from trl import SFTTrainer, SFTConfig

# ================= é…ç½®åŒºåŸŸ =================
DATA_FILE = "generated_hoshino_data.jsonl"  # ç¡®ä¿è¿™é‡ŒæŒ‡å‘æ‚¨çš„æ•°æ®æ–‡ä»¶
MODEL_ID = "Qwen/Qwen2.5-7B-Instruct"
OUTPUT_DIR = "./generated_hoshino_data"


# ===========================================

def train():
    # 1. åŠ è½½ tokenizer
    print("Processing tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token

    # 2. åŠ è½½æ•°æ®é›†
    print("Loading dataset...")
    try:
        dataset = load_dataset("json", data_files=DATA_FILE, split="train")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return

    # =======================================================
    # ğŸ› ï¸ ä¿®æ”¹åŒºåŸŸï¼šé€‚é…æ ‡å‡† "messages" æ ¼å¼
    # =======================================================
    print("Formatting dataset (Standard Messages -> ChatML)...")

    def format_data_to_text(row):
        # 1. ç›´æ¥è¯»å– "messages" å­—æ®µ
        # ä½ çš„æ•°æ®å·²ç»æ˜¯æ ‡å‡†çš„ [{"role": "system/user/assistant", "content": "..."}, ...] æ ¼å¼
        messages = row.get("messages", [])

        # 2. ç›´æ¥åº”ç”¨èŠå¤©æ¨¡æ¿
        # Qwen2.5 çš„æ¨¡æ¿ä¼šè‡ªåŠ¨å¤„ç† system, user, assistant è§’è‰²
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        return {"text": text}

    # åº”ç”¨è½¬æ¢
    try:
        dataset = dataset.map(format_data_to_text)
    except Exception as e:
        print(f"âŒ æ•°æ®æ ¼å¼è½¬æ¢å¤±è´¥ã€‚è¯·æ£€æŸ¥æ‚¨çš„ JSONL æ–‡ä»¶æ˜¯å¦åŒ…å« 'messages' å­—æ®µã€‚\né”™è¯¯ä¿¡æ¯: {e}")
        return

    # æ‰“å°ç¤ºä¾‹ä»¥ä¾›æ£€æŸ¥
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼æ ·æœ¬ç¤ºä¾‹:\n{dataset[0]['text'][:200]}...")

    # 3. æ¨¡å‹å‡†å¤‡ (QLoRA)
    print("Loading model...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )

    # 4. LoRA é…ç½®
    peft_config = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )

    # 5. è®­ç»ƒå‚æ•°
    training_args = SFTConfig(
        output_dir=OUTPUT_DIR,
        num_train_epochs=3,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        logging_steps=5,
        save_strategy="epoch",
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        optim="paged_adamw_8bit",
        packing=False,
    )

    # æŒ‡å®šå­—æ®µä¸º "text"
    training_args.dataset_text_field = "text"
    training_args.max_seq_length = 2048  # ç¨å¾®è°ƒå¤§ä¸€ç‚¹ä»¥å®¹çº³å¤šè½®å¯¹è¯

    # 6. åˆå§‹åŒ– Trainer
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        peft_config=peft_config,
        processing_class=tokenizer,
    )

    # 7. å¼€å§‹è®­ç»ƒ
    print("ğŸš€ Starting training...")
    trainer.train()

    # 8. ä¿å­˜
    print(f"âœ… Done! Model saved to {OUTPUT_DIR}")
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)


if __name__ == "__main__":
    train()