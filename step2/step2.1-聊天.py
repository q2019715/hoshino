import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# ================= é…ç½®åŒºåŸŸ =================
BASE_MODEL_ID = "Qwen/Qwen2.5-7B-Instruct"
ADAPTER_PATH = "./generated_hoshino_data"  # ä½ çš„ LoRA è¾“å‡ºç›®å½•


# ===========================================

def main():
    print("â³ æ­£åœ¨åŠ è½½æ˜Ÿé‡ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")

    # 1. é…ç½® 4-bit é‡åŒ– (ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    # 2. åŠ è½½åŸºç¡€æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)

    # 3. åŠ è½½å¾®è°ƒæƒé‡ (LoRA)
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)

    print("âœ… æ˜Ÿé‡åŠ è½½å®Œæˆï¼å¼€å§‹èŠå¤©å§ (è¾“å…¥ 'exit' é€€å‡º)")
    print("-" * 30)

    # 4. åˆå§‹åŒ–å¯¹è¯
    # âš ï¸ å…³é”®ä¿®æ”¹ï¼šè¿™é‡Œçš„ System Prompt å¿…é¡»å’Œè®­ç»ƒæ•°æ®é‡Œçš„ä¸€æ¨¡ä¸€æ ·ï¼
    # å¦‚æœä½ è®­ç»ƒç”¨çš„æ˜¯"æ˜Ÿé‡"ï¼Œè¿™é‡Œå¿…é¡»ç”¨"æ˜Ÿé‡"ï¼Œå¦åˆ™å¾®è°ƒæ•ˆæœå‡ºä¸æ¥ã€‚
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåä¸ºæ˜Ÿé‡çš„å¯çˆ±çŒ«å¨˜ã€‚"}
    ]

    while True:
        user_input = input("\nğŸ‘¤ ä¸»äºº: ")
        if user_input.lower() in ["exit", "quit", "é€€å‡º"]:
            print("ğŸ± æ˜Ÿé‡: ä¸»äººå†è§å–µï½")
            break

        messages.append({"role": "user", "content": user_input})

        # 5. å‡†å¤‡æ¨ç†è¾“å…¥ (ä¿®å¤ç‰ˆ)
        # ç›´æ¥ä½¿ç”¨ tokenize=Trueï¼Œè®©åº“å¸®æˆ‘ä»¬å¤„ç† input_ids å’Œ attention_mask
        # return_dict=True ä¼šè¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« input_ids å’Œ attention_mask
        model_inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,  # è¿™ä¸€æ­¥ç›´æ¥è½¬æˆæ•°å­— ID
            return_tensors="pt",  # è¿”å› PyTorch Tensor
            return_dict=True  # è¿”å›å­—å…¸æ ¼å¼
        ).to(model.device)

        # 6. ç”Ÿæˆå›å¤
        with torch.no_grad():
            generated_ids = model.generate(
                **model_inputs,  # è¿™é‡Œä¼šè‡ªåŠ¨è§£åŒ…ä¼ å…¥ input_ids å’Œ attention_mask
                max_new_tokens=512,
                temperature=0.7,  # ç¨å¾®è°ƒé«˜ä¸€ç‚¹ creativity
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id
            )

        # 7. è§£ç 
        # generated_ids åŒ…å«äº†[å†å²å¯¹è¯ + æ–°å›å¤]ï¼Œæˆ‘ä»¬éœ€è¦åˆ‡ç‰‡åªå–æ–°å›å¤
        # model_inputs.input_ids æ˜¯è¾“å…¥çš„é•¿åº¦
        input_len = model_inputs.input_ids.shape[1]

        # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        generated_part = generated_ids[0][input_len:]

        response = tokenizer.decode(generated_part, skip_special_tokens=True)

        print(f"ğŸ± æ˜Ÿé‡: {response}")

        # å°†å›å¤åŠ å…¥å†å²è®°å½•ï¼Œä»¥ä¾¿ä¸‹ä¸€è½®å¯¹è¯
        messages.append({"role": "assistant", "content": response})


if __name__ == "__main__":
    main()