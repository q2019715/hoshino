import os
import sys
import yaml
import json
import random
import requests
import time
import argparse
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ==========================================
# ğŸ”§ å¾®è°ƒä¸“ç”¨é…ç½® (åœ¨æ­¤å¤„ä¿®æ”¹ä¿å­˜åˆ°æ–‡ä»¶é‡Œçš„ System Prompt)
# ==========================================
# è¿™å¥è¯ä¼šå†™å…¥ output æ–‡ä»¶ã€‚å¾®è°ƒæ—¶ï¼Œæ¨¡å‹çœ‹åˆ°è¿™å¥è¯å°±ä¼šæ¿€æ´»çŒ«å¨˜æ¨¡å¼ã€‚
# å¦‚æœä½ æƒ³è®© system ä¸ºç©ºï¼Œå¯ä»¥è®¾ç½®: FINETUNE_SYSTEM_PROMPT = ""
FINETUNE_SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªåä¸ºæ˜Ÿé‡çš„å¯çˆ±çŒ«å¨˜ã€‚"

# å…¨å±€æ–‡ä»¶é”
file_write_lock = threading.Lock()

# ==========================================
# 1. é…ç½®ä¸å·¥å…·å‡½æ•°
# ==========================================

def load_config(config_path="config.yaml"):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ '{config_path}' æœªæ‰¾åˆ°ã€‚")
        sys.exit(1)

def load_seed_questions(seed_file_path):
    """åŠ è½½JSONæ ¼å¼çš„é—®é¢˜ç§å­æ–‡ä»¶"""
    try:
        with open(seed_file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: é—®é¢˜ç§å­æ–‡ä»¶ '{seed_file_path}' æœªæ‰¾åˆ°ã€‚")
        sys.exit(1)

def check_api_availability(config):
    """æ£€æŸ¥APIå¯ç”¨æ€§"""
    print("æ­£åœ¨æ‰§è¡Œ API å¯ç”¨æ€§æ£€æŸ¥...")
    api_conf = config['api_config']
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_conf['api_key']}"}
    payload = {
        "model": api_conf['model_name'], 
        "messages": [{"role": "user", "content": "ping"}], 
        "max_tokens": 5
    }
    
    try:
        response = requests.post(api_conf['base_url'], headers=headers, json=payload, timeout=10)
        response.raise_for_status()
        print(f"âœ… API è¿æ¥æˆåŠŸ! æ¨¡å‹ '{api_conf['model_name']}' å¯ç”¨ã€‚")
        return True
    except Exception as e:
        print(f"âŒ API æ£€æŸ¥å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥ config.yaml ä¸­çš„ base_url å’Œ api_keyã€‚")
        return False

def call_llm_api(config, messages):
    """è°ƒç”¨APIï¼ŒåŒ…å«é‡è¯•å’Œå»¶æ—¶é€»è¾‘"""
    api_conf = config['api_config']
    max_retries = api_conf.get('max_retries', 2)
    
    for attempt in range(max_retries + 1):
        try:
            delay = api_conf.get('request_delay_seconds', 1.0)
            if delay > 0:
                time.sleep(delay)

            headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_conf['api_key']}"}
            payload = {
                "model": api_conf['model_name'], 
                "messages": messages, 
                "temperature": 0.8, 
                "max_tokens": 2048
            }

            response = requests.post(api_conf['base_url'], headers=headers, json=payload, timeout=120)
            response.raise_for_status()
            
            return response.json()['choices'][0]['message']['content'].strip()

        except Exception as e:
            if attempt == max_retries:
                return None
    return None

# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘ï¼šå•æ¡æ•°æ®å¤„ç†
# ==========================================

def evolve_question(config, original_question):
    prompt = config['evolver_config']['evolution_prompt'].format(original_question=original_question)
    messages = [{"role": "user", "content": prompt}]
    return call_llm_api(config, messages)

def generate_next_question(config, conversation_history):
    prompt = config['generation_config']['next_question_prompt']
    messages = []
    for turn in conversation_history:
        messages.append({"role": "user", "content": turn['role_user']})
        messages.append({"role": "assistant", "content": turn['role_assistant']})
    messages.append({"role": "user", "content": prompt})
    return call_llm_api(config, messages)

def process_single_seed(config, question, output_file_handle):
    gen_conf = config['generation_config']
    # è¿™é‡Œè¯»å–çš„æ˜¯ config.yaml é‡Œçš„è¶…é•¿äººè®¾ï¼Œä»…ç”¨äº API è°ƒç”¨ï¼ˆæ‰®æ¼”ï¼‰
    actor_prompt = config['persona_config']['system_prompt']

    # 1. è¿›åŒ–é—®é¢˜
    evolved_q = evolve_question(config, question)
    if not evolved_q: 
        return False

    # 2. ç”Ÿæˆå¤šè½®å¯¹è¯
    num_turns = random.randint(gen_conf['min_turns'], gen_conf['max_turns'])
    history = []
    current_q = evolved_q
    conversation_valid = True
    
    for i in range(num_turns):
        # API è°ƒç”¨æ—¶ï¼šä½¿ç”¨é•¿ System Prompt
        msgs = [{"role": "system", "content": actor_prompt}]
        for turn in history:
            msgs.append({"role": "user", "content": turn['role_user']})
            msgs.append({"role": "assistant", "content": turn['role_assistant']})
        msgs.append({"role": "user", "content": current_q})
        
        answer = call_llm_api(config, msgs)
        if not answer:
            conversation_valid = False; break
        
        history.append({"role_user": current_q, "role_assistant": answer})
        
        if i < num_turns - 1:
            next_q = generate_next_question(config, history)
            if not next_q:
                conversation_valid = False; break
            current_q = next_q

    # 3. æ ¼å¼åŒ–å¹¶ä¿å­˜ (å…³é”®ä¿®æ”¹ç‚¹!)
    if conversation_valid and len(history) >= gen_conf.get('min_turns', 1):
        
        # ä¿å­˜æ—¶ï¼šä½¿ç”¨çŸ­ System Prompt (æˆ–è€…ç©º)
        # å¦‚æœ FINETUNE_SYSTEM_PROMPT æœ‰å†…å®¹ï¼Œå°±åŠ ä¸Š system message
        if FINETUNE_SYSTEM_PROMPT:
            standard_messages = [{"role": "system", "content": FINETUNE_SYSTEM_PROMPT}]
        else:
            standard_messages = [] # å¦‚æœä¸ºç©ºï¼Œåˆ™ä¸ä¿å­˜ system å­—æ®µ
            
        for turn in history:
            standard_messages.append({"role": "user", "content": turn['role_user']})
            standard_messages.append({"role": "assistant", "content": turn['role_assistant']})
        
        output_data = {"messages": standard_messages}
        json_line = json.dumps(output_data, ensure_ascii=False) + "\n"
        
        with file_write_lock:
            output_file_handle.write(json_line)
            output_file_handle.flush()
        
        return True
    return False

# ==========================================
# 3. ä¸»é€»è¾‘æ§åˆ¶å™¨
# ==========================================

def run_data_generation(config, max_workers=3):
    if not check_api_availability(config):
        return

    seed_questions = load_seed_questions(config['file_config']['seed_file'])
    output_file = config['file_config']['output_file']
    
    processed_count = 0
    if os.path.exists(output_file):
        with open(output_file, 'r', encoding='utf-8') as f:
            processed_count = sum(1 for _ in f)
    
    if processed_count > 0:
        print(f"ğŸ“„ æ£€æµ‹åˆ°å·²ç”Ÿæˆ {processed_count} æ¡æ•°æ®ï¼Œå°†è·³è¿‡è¿™äº›ç§å­...")
        seed_questions = seed_questions[processed_count:]

    if not seed_questions:
        print("âœ¨ æ‰€æœ‰ç§å­å·²å¤„ç†å®Œæ¯•ï¼")
        return

    print(f"\nğŸš€ å¼€å§‹å¤šçº¿ç¨‹ç”Ÿæˆä»»åŠ¡ (å¹¶å‘æ•°: {max_workers})...")
    print(f"ğŸ’¾ ä¿å­˜æ—¶çš„ System Prompt å°†è¢«æ›¿æ¢ä¸º: '{FINETUNE_SYSTEM_PROMPT}'")
    
    with open(output_file, 'a', encoding='utf-8') as f:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(process_single_seed, config, q, f) 
                for q in seed_questions
            ]
            
            success_count = 0
            for future in tqdm(as_completed(futures), total=len(seed_questions), desc="å¹¶å‘å¤„ç†ä¸­"):
                try:
                    if future.result():
                        success_count += 1
                except Exception:
                    pass

    print(f"\nâœ… ä»»åŠ¡å®Œæˆï¼æœ¬æ¬¡æˆåŠŸç”Ÿæˆ: {success_count} æ¡ã€‚")
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³: {output_file}")

# ==========================================
# 4. äº¤äº’æ¨¡å¼
# ==========================================

def run_interactive_chat(config):
    if not check_api_availability(config): return
    persona_prompt = config['persona_config']['system_prompt']
    history = []
    print("\n--- ğŸ± æ˜Ÿé‡æµ‹è¯•ç»ˆç«¯ ---")
    while True:
        try:
            user_input = input("\nğŸ‘¤ ä¸»äºº: ")
            if user_input.lower() in ['exit', 'quit']: break
            
            messages = [{"role": "system", "content": persona_prompt}] + history + [{"role": "user", "content": user_input}]
            print("â³ æ€è€ƒä¸­...", end="", flush=True)
            response = call_llm_api(config, messages)
            print("\r" + " " * 20 + "\r", end="")

            if response:
                print(f"ğŸ± æ˜Ÿé‡: {response}")
                history.append({"role": "user", "content": user_input})
                history.append({"role": "assistant", "content": response})
        except KeyboardInterrupt:
            break

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--chat", action="store_true", help="äº¤äº’æ¨¡å¼")
    parser.add_argument("--workers", type=int, default=3, help="å¹¶å‘æ•°")
    args = parser.parse_args()
    config = load_config()

    if args.chat:
        run_interactive_chat(config)
    else:
        run_data_generation(config, max_workers=args.workers)