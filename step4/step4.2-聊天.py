import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# ================= é…ç½®åŒºåŸŸ =================
BASE_MODEL_ID = "Qwen/Qwen2.5-7B-Instruct"
ADAPTER_PATH = "./generated_hoshino_v2"  # ä½ çš„ LoRA è¾“å‡ºç›®å½•

# é¢„è®¾æç¤ºè¯åº“
PRESET_PROMPTS = {
    "1": {
        "name": "æ˜Ÿé‡çŒ«å¨˜",
        "prompt": "ä½ æ˜¯ä¸€ä¸ªåä¸ºæ˜Ÿé‡çš„å¯çˆ±çŒ«å¨˜ã€‚"
    },
    "2": {
        "name": "ä¸“ä¸šåŠ©æ‰‹",
        "prompt": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„AIåŠ©æ‰‹ï¼Œæ“…é•¿æä¾›å‡†ç¡®çš„ä¿¡æ¯å’Œå»ºè®®ã€‚"
    },
    "3": {
        "name": "å¹½é»˜ä¼™ä¼´",
        "prompt": "ä½ æ˜¯ä¸€ä¸ªå¹½é»˜é£è¶£çš„èŠå¤©ä¼™ä¼´ï¼Œå–œæ¬¢ç”¨è½»æ¾æ„‰å¿«çš„æ–¹å¼äº¤æµã€‚"
    },
    "4": {
        "name": "å­¦ä¹ å¯¼å¸ˆ",
        "prompt": "ä½ æ˜¯ä¸€ä¸ªè€å¿ƒçš„å­¦ä¹ å¯¼å¸ˆï¼Œæ“…é•¿ç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šå¤æ‚æ¦‚å¿µã€‚"
    },
    "5": {
        "name": "åˆ›æ„å†™æ‰‹",
        "prompt": "ä½ æ˜¯ä¸€ä¸ªå¯Œæœ‰åˆ›æ„çš„å†™æ‰‹ï¼Œæ“…é•¿åˆ›ä½œæ•…äº‹ã€è¯—æ­Œå’Œå„ç§æ–‡å­¦ä½œå“ã€‚"
    }
}


# ===========================================

def select_system_prompt():
    """è®©ç”¨æˆ·é€‰æ‹©æˆ–è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯"""
    print("\n" + "=" * 50)
    print("ğŸ­ è¯·é€‰æ‹©ç³»ç»Ÿæç¤ºè¯:")
    print("=" * 50)

    # æ˜¾ç¤ºé¢„è®¾é€‰é¡¹
    for key, value in PRESET_PROMPTS.items():
        print(f"  [{key}] {value['name']}")
        print(f"      â†’ {value['prompt']}")
        print()

    print(f"  [0] è‡ªå®šä¹‰æç¤ºè¯")
    print("=" * 50)

    while True:
        choice = input("\nğŸ‘‰ è¯·è¾“å…¥é€‰é¡¹ç¼–å· (0-5): ").strip()

        if choice == "0":
            # è‡ªå®šä¹‰æç¤ºè¯
            custom_prompt = input("\nâœï¸  è¯·è¾“å…¥ä½ çš„è‡ªå®šä¹‰æç¤ºè¯: ").strip()
            if custom_prompt:
                print(f"\nâœ… å·²è®¾ç½®è‡ªå®šä¹‰æç¤ºè¯: {custom_prompt}")
                return custom_prompt
            else:
                print("âŒ æç¤ºè¯ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
                continue

        elif choice in PRESET_PROMPTS:
            selected = PRESET_PROMPTS[choice]
            print(f"\nâœ… å·²é€‰æ‹©: {selected['name']}")
            print(f"   æç¤ºè¯: {selected['prompt']}")
            return selected['prompt']

        else:
            print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·è¾“å…¥ 0-5 ä¹‹é—´çš„æ•°å­—")


def main():
    print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")

    # 1. é…ç½® 4-bit é‡åŒ–
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    # 2. åŠ è½½åŸºç¡€æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)

    # 3. åŠ è½½å¾®è°ƒæƒé‡ (LoRA)
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)

    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    # 4. è®©ç”¨æˆ·é€‰æ‹©ç³»ç»Ÿæç¤ºè¯
    system_prompt = select_system_prompt()

    # 5. åˆå§‹åŒ–å¯¹è¯
    messages = [
        {"role": "system", "content": system_prompt}
    ]

    print("\n" + "=" * 50)
    print("ğŸ’¬ å¼€å§‹èŠå¤©å§ï¼")
    print("   è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    print("   è¾“å…¥ 'reset' é‡æ–°é€‰æ‹©æç¤ºè¯")
    print("   è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    print("=" * 50)

    while True:
        user_input = input("\nğŸ‘¤ ä½ : ").strip()

        if user_input.lower() in ["exit", "quit", "é€€å‡º"]:
            print("\nğŸ‘‹ å†è§ï¼")
            break

        # é‡æ–°é€‰æ‹©æç¤ºè¯
        if user_input.lower() == "reset":
            system_prompt = select_system_prompt()
            messages = [{"role": "system", "content": system_prompt}]
            print("\nâœ… æç¤ºè¯å·²æ›´æ–°ï¼Œå¯¹è¯å†å²å·²æ¸…ç©º")
            continue

        # æ¸…ç©ºå¯¹è¯å†å²
        if user_input.lower() == "clear":
            messages = [{"role": "system", "content": system_prompt}]
            print("\nâœ… å¯¹è¯å†å²å·²æ¸…ç©º")
            continue

        if not user_input:
            continue

        messages.append({"role": "user", "content": user_input})

        # 6. å‡†å¤‡æ¨ç†è¾“å…¥
        model_inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_tensors="pt",
            return_dict=True
        ).to(model.device)

        # 7. ç”Ÿæˆå›å¤
        with torch.no_grad():
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id
            )

        # 8. è§£ç 
        input_len = model_inputs.input_ids.shape[1]
        generated_part = generated_ids[0][input_len:]
        response = tokenizer.decode(generated_part, skip_special_tokens=True)

        print(f"\nğŸ¤– AI: {response}")

        # å°†å›å¤åŠ å…¥å†å²è®°å½•
        messages.append({"role": "assistant", "content": response})


if __name__ == "__main__":
    main()
